{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "motion_exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pojo-25/AdvancedTensorflow/blob/main/motion_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9mB2guAbHGW"
      },
      "source": [
        "# A Simple Exercise on Unsupervised ML / Anomaly Detection\n",
        "*Confidentiality Notice*: Distribution of this material to anyone without explicit written consent of Synnada, Inc. is not allowed.\n",
        "\n",
        "Copyright â’¸ Synnada, Inc., All Rights Reserved.\n",
        "\n",
        "## Context\n",
        "\n",
        "In this exercise, you will get to develop your own algorithm(s), or apply any unsupervised ML and/or anomaly detection technique(s) of your choice, to crack a (very) low-dimensional toy problem that will give you a small taste of the bigger problems we will be dealing with at Synnada.\n",
        "\n",
        "## The Problem\n",
        "\n",
        "We will be dealing with several scenarios where we observe objects whose \"usual\" motions exhibit some regularity. In every scenario we study, these objects will temporarily deviate from their usual trajectories. For example, they may go off-course or change their speed and/or direction.\n",
        "\n",
        "The task is to create an algorithm that takes in simple observations (i.e. triples of the form (timestamp, x-coordinate measurement, y-coordinate measurement)) **one by one**, updates its state, and outputs a score value that correlates with how \"surprising\" the observation is. You will use these scores to announce alerts when the score is above (or below) a threshold of your choice. Your algorithm should have *at least* the following two hyper-parameters:\n",
        "1. The alerting threshold.\n",
        "2. The warm-up duration; i.e. the time interval during which your algorithm does not announce *any* alerts. During this time, the algorithm should just take in observations to update its state and learn.\n",
        "\n",
        "Once you have your algorithm (or algorithms; if you work on multiple approaches) ready, apply your favorite hyper-parameter sweeping/optimization technique to explore the false positive/negative trade-off of your algorithm(s). We suggest you to treat the alerting threshold separately from the other hyper-parameters and compute a threshold-independent metric such as the [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#:~:text=ROC%20curve%2C%20or%20%22-,AUC,-%22%20(%22area%20under%20curve) to guide your search.\n",
        "\n",
        "Finally, do not forget to comment on the computational (time and memory) complexity and/or scalability of your approach(es).\n",
        "\n",
        "## The Notebook\n",
        "\n",
        "This notebook starts with various function definitions, which enable us to construct and visualize the scenarios we will study. We **strongly** suggest you to take a look at them for three reasons:\n",
        "1. You will be able to use these functions to construct scenarios of your own in order to test your algorithms and push them to their limits.\n",
        "2. You can use the animation code as a template to construct useful visualizations when debugging/analyzing your algorithms.\n",
        "3. Studying our code should give you an idea on our style of software engineering here at Synnada.\n",
        "\n",
        "Next, the notebook offers three example scenarios you can use to get going.\n",
        "\n",
        "Finally, we have a section that contains the skeleton code you will use to develop your algorithm and run simulations.\n",
        "\n",
        "## Warnings and Suggestions\n",
        "\n",
        "Although we are confident that you are already aware of the usual mistakes to steer away from, it is probably still worth to make a few remarks/suggestions:\n",
        "- Avoid overfitting to specific trajectory shapes. You can test whether this happens by constructing new scenarios.\n",
        "- If you decide to do hyper-parameter optimization, and do not already have a favorite library for this purpose, we suggest you to check out [Optuna](https://optuna.org).\n",
        "- Explore how your algorithm(s) behave(s) as you increase the warm-up duration. Tweak the scenarios so that the object spends more time in its \"usual\" motion and vary your warm-up duration. You should be able to improve your false positive/negative characteristics by lengthening your warm-up duration. If that is not the case, you may be doing something wrong.\n",
        "- Explore the noise tolerance of your techniques. How do things change with increasing/decreasing noise? If what you see during your analysis is surprising, you may be doing something wrong.\n",
        "- Explore how your algorithm(s) react as you change the decision boundary (i.e. the alerting threshold). If what you see surprises you, there may be a bug somewhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrpRBP8lA3o9"
      },
      "source": [
        "from typing import Tuple, Dict, List, Callable, Any, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Type definition for functions that operate on NumPy arrays:\n",
        "UFunc = Callable[[np.ndarray], np.ndarray]\n",
        "\n",
        "def generate_sampling_times(duration: float,                            \n",
        "                            *,\n",
        "                            avg_sampling_interv: float = 1.0,\n",
        "                            include_first: bool = True,\n",
        "                            include_last: bool = False) -> np.ndarray:\n",
        "  # This function returns an array of timestamps with Poisson arrivals.\n",
        "\n",
        "  # The argument \"duration\" specifies the total simulation time.\n",
        "  # The argument \"avg_sampling_interv\" specifies the average sampling interval.\n",
        "  # Arguments \"include_first\" and \"include_last\" specify whether we include\n",
        "  # initial and final timestamps in the result.\n",
        "\n",
        "  # Validate arguments:\n",
        "  if duration <= 0:\n",
        "    raise ValueError('Argument \"duration\" has to be positive!')  \n",
        "  if avg_sampling_interv <= 0:\n",
        "    raise ValueError('Argument \"avg_sampling_interv\" has to be positive!')\n",
        "\n",
        "  # We can simulate Poisson arrivals by accumulating exponential intervals.\n",
        "  ts, times = 0.0, []\n",
        "  if include_first:\n",
        "    times.append(0.0)\n",
        "  # Generate samples until we hit the finish timestamp:\n",
        "  while ts < duration:\n",
        "    ts += np.random.exponential(avg_sampling_interv)\n",
        "    if ts <= duration:\n",
        "      times.append(ts)\n",
        "  # If applicable, add the final timestamp:\n",
        "  if include_last and not (times and (times[-1] == duration)):\n",
        "    times.append(duration)\n",
        "  return np.array(times)\n",
        "\n",
        "def add_noise(trace: np.ndarray,\n",
        "              noise_magnitude: float,\n",
        "              *,\n",
        "              inplace: bool = False) -> np.ndarray:\n",
        "  # This function superimposes additive white Gaussian noise on a given array\n",
        "  # of measurements (i.e. [(ts, x, y), ...]). Both coordinates couple with the\n",
        "  # noise independently.\n",
        "\n",
        "  # The argument \"trace\" denotes the array of measurements.\n",
        "  # The argument \"noise_magnitude\" specifies the noise standard deviation.\n",
        "  # The argument \"inplace\" denotes whether the addition occurs in-place.\n",
        "\n",
        "  # Validate arguments:\n",
        "  if noise_magnitude < 0:\n",
        "    raise ValueError('Argument \"noise_magnitude\" has to be non-negative!')  \n",
        "\n",
        "  # Operate on a copy if necessary:\n",
        "  if not inplace:\n",
        "    trace = trace.copy()\n",
        "  if noise_magnitude > 0:\n",
        "    rows, cols = trace.shape\n",
        "    view_shape = (rows, cols - 1)\n",
        "    # Add noise to all columns except the first, which contains timestamps:\n",
        "    trace[:, 1: ] += np.random.normal(0.0, noise_magnitude, view_shape)\n",
        "  return trace\n",
        "\n",
        "def create_template_trace(duration: float,\n",
        "                          position_gen: UFunc,\n",
        "                          *,\n",
        "                          start_ts: float = 0.0,\n",
        "                          noise_magnitude: float = 0.0,\n",
        "                          **kwargs) -> np.ndarray:\n",
        "  # This function generates an array of triples (i.e. [(ts, x, y), ...]) where\n",
        "  # entries denote positional measurements of an object moving on some curve.\n",
        "\n",
        "  # The argument \"position_gen\" is a function that takes in an array of times\n",
        "  # (starting with zero) and generates an array of noise-free observations on\n",
        "  # the curve.\n",
        "  # The argument \"start_ts\" denotes the initial timestamp to use as an offset.  \n",
        "  # The argument \"noise_magnitude\" specifies the magnitude of additive white\n",
        "  # Gaussian noise in our observations.\n",
        "  # See the function \"generate_sampling_times\" for keyword-arguments.\n",
        "\n",
        "  # Generate timestamps:\n",
        "  times = generate_sampling_times(duration, **kwargs)\n",
        "  # Generate the trace without measurement noise:    \n",
        "  trace = position_gen(times)\n",
        "  # Offset timestamps appropriately:\n",
        "  trace[:, 0] += start_ts\n",
        "  # Add measurement noise if applicable:\n",
        "  return add_noise(trace, noise_magnitude, inplace = True)\n",
        "\n",
        "def create_circular_trace(duration: float,\n",
        "                          *,\n",
        "                          init_pos: Tuple[float, float] = (1.0, 0.0),\n",
        "                          speed: float = 1.0,                          \n",
        "                          **kwargs) -> np.ndarray:\n",
        "  # This function computes an array of triples (i.e. [(ts, x, y), ...]) where\n",
        "  # entries denote positional measurements of an object in uniform circular\n",
        "  # motion around the origin.\n",
        "\n",
        "  # The argument \"init_pos\" specifies the starting position.\n",
        "  # The argument \"speed\" specifies the speed of the object. Negative (positive)\n",
        "  # values imply (counter-)clockwise motion.\n",
        "  # See the function \"create_template_trace\" for keyword-arguments.\n",
        "\n",
        "  # Calculate initial radius and angle:\n",
        "  init_x, init_y = init_pos\n",
        "  radius = np.sqrt(init_x ** 2 + init_y ** 2)\n",
        "  angle = np.arctan2(init_y, init_x)\n",
        "  omega = speed / radius\n",
        "  # Verify that we sample frequently enough:\n",
        "  if kwargs.get('avg_sampling_interv', 1.0) > abs(np.pi / omega):\n",
        "    raise ValueError('Argument \"arg_sampling_interv\" implies sub-Nyquist sampling!')\n",
        "\n",
        "  # Define the function that generates the curve:\n",
        "  def circle(times: np.ndarray) -> np.ndarray:\n",
        "    x = radius * np.cos(angle + omega * times)\n",
        "    y = radius * np.sin(angle + omega * times)\n",
        "    return np.vstack((times, x, y)).T\n",
        "\n",
        "  return create_template_trace(duration, circle, **kwargs)\n",
        "\n",
        "def create_linear_trace(duration: float,\n",
        "                        *,\n",
        "                        init_pos: Tuple[float, float] = (0.0, 0.0),\n",
        "                        velocity: Tuple[float, float] = (1.0, 0.0),\n",
        "                        **kwargs) -> np.ndarray:\n",
        "  # This function computes an array of triples (i.e. [(ts, x, y), ...]) where\n",
        "  # entries denote positional measurements of an object in uniform linear\n",
        "  # motion.\n",
        "\n",
        "  # The argument \"init_pos\" specifies the starting position.\n",
        "  # The argument \"velocity\" specifies the (vector) velocity of the object. \n",
        "  # See the function \"create_template_trace\" for keyword-arguments.\n",
        "\n",
        "  # Define the function that generates the curve:\n",
        "  def line(times: np.ndarray) -> np.ndarray:\n",
        "    x = init_pos[0] + velocity[0] * times\n",
        "    y = init_pos[1] + velocity[1] * times\n",
        "    return np.vstack((times, x, y)).T\n",
        "\n",
        "  return create_template_trace(duration, line, **kwargs)\n",
        "\n",
        "def concat_traces(trace_configs: Tuple[str, Dict[str, Any]],\n",
        "                  **kwargs) -> np.ndarray:\n",
        "  # This function creates a composite trace by concatenating multiple simple\n",
        "  # traces. As before, it outputs an array of triples (i.e. [(ts, x, y), ...]).\n",
        "  # Each constituent simple trace begins where its predecessor ends.\n",
        "\n",
        "  # The argument \"trace_configs\" contains the arguments to use when generating\n",
        "  # constituent trace/segments.\n",
        "  # See the function \"create_template_trace\" for keyword-arguments. These\n",
        "  # arguments apply to all constituent traces as default values in case the\n",
        "  # argument \"trace_configs\" does not specify things with fine granularity.\n",
        "\n",
        "  # Example usage:\n",
        "  #\n",
        "  # concat_traces(trace_configs = [\n",
        "  #   ('circular', {'duration': 6.0 * np.pi, 'init_pos': (1.0, 0.0)}),\n",
        "  #   ('circular', {'duration': 6.0 * np.pi, 'speed': anomaly_speed}),\n",
        "  #   ('circular', {'duration': 3.0 * np.pi})\n",
        "  #  ],\n",
        "  #  avg_sampling_interv = 0.1,\n",
        "  #  noise_magnitude = 0.02\n",
        "  # )\n",
        "\n",
        "  def use_trace(*, data: np.ndarray, start_ts: float, **kwargs) -> np.ndarray:\n",
        "    # This function enables us to use a given trace as a component of the\n",
        "    # composite trace we are building.\n",
        "    data = data.copy()\n",
        "    data[:, 0] += start_ts\n",
        "    return data\n",
        "\n",
        "  # We support the following elementary traces:\n",
        "  trace_dict = {\n",
        "    'circular': create_circular_trace,\n",
        "    'linear': create_linear_trace,\n",
        "    'immediate': use_trace\n",
        "  }\n",
        "  segments, current_ts, current_pos = [], 0.0, (0.0, 0.0)\n",
        "  # Use given trace configurations to generate elementary segments:  \n",
        "  for idx, (shape, args) in enumerate(trace_configs):\n",
        "    # Construct the trace configuration for the current segment:\n",
        "    spec = {**kwargs, **args}\n",
        "    spec.setdefault('start_ts', current_ts)\n",
        "    spec.setdefault('init_pos', current_pos)\n",
        "    spec.setdefault('include_first', idx == 0)\n",
        "    spec.setdefault('include_last', True)\n",
        "    # Save the applicable noise magnitude for the current segment:\n",
        "    noise_mag = spec.pop('noise_magnitude', 0.0)\n",
        "    # Note that we generate the current segment without noise first. We have to\n",
        "    # do this; because concatenating noisy segments introduces drifts. Once we\n",
        "    # compute a \"noiseless\" endpoint for the segment, we can superimpose noise.\n",
        "    segment = trace_dict[shape](**spec)\n",
        "    # Save the endpoint of this segment, it will be the next initial point:\n",
        "    current_ts, *current_pos = segment[-1, :]\n",
        "    # Superimpose noise on the current segment:\n",
        "    segment = add_noise(segment, noise_mag, inplace = True)\n",
        "    segments.append(segment)\n",
        "\n",
        "  return np.concatenate(segments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xow8dvX5ZhUQ"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
        "\n",
        "def create_animation(trace: np.ndarray,\n",
        "                     min_x: float, max_x: float,\n",
        "                     min_y: float, max_y: float):\n",
        "  # This function creates an animation of the given trace, which is a list of\n",
        "  # triples of the form [(ts, x, y), ...] where every entry corresponds to a\n",
        "  # positional observation of an object moving on some curve.\n",
        "\n",
        "  # Setup the baseline figure, the axis and the limits:\n",
        "  fig = plt.figure()\n",
        "  ax = plt.gca()\n",
        "  ax.set_xlim((min_x, max_x))\n",
        "  ax.set_ylim((min_y, max_y))\n",
        "  ax.set_aspect('equal', 'box')\n",
        "  txt_title = fig.suptitle('')\n",
        "  # Initialize an empty scatter plot:\n",
        "  sc_history = plt.scatter([], [], c = 'red')\n",
        "  sc_current = plt.scatter([], [], c = 'black')\n",
        "\n",
        "  def draw_frame(frame_idx: int):\n",
        "    # This function runs every frame. Using ever-growing array slices, we\n",
        "    # progressively show more samples every frame.\n",
        "    sc_history.set_offsets(trace[: frame_idx, 1: ])\n",
        "    sc_current.set_offsets(trace[frame_idx, 1: ])\n",
        "    txt_title.set_text('Observation #{0:4d}'.format(frame_idx))\n",
        "    return (sc_history, sc_current)\n",
        "\n",
        "  # Create the actual animation object:\n",
        "  result = animation.FuncAnimation(fig,\n",
        "                                   draw_frame,\n",
        "                                   frames = trace.shape[0],\n",
        "                                   interval = 100,\n",
        "                                   blit = True)\n",
        "  # The plotting library spuriously plots the baseline figure, inhibit that:\n",
        "  plt.close()\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaMH9S73QzAw"
      },
      "source": [
        "## Scenarios\n",
        "\n",
        "Below, we define functions that generate observational datasets for three example scenarios (i.e. functions `scenario_XXX`). These functions return two arrays:\n",
        "- The observations,\n",
        "- A mask indicating which observations are anomalous.\n",
        "\n",
        "**Do not** interpret/use these masks as supervision signals (i.e. labels). We are generating this information so that you can quantify how well your algorithms do -- not for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rclCo00AyW7"
      },
      "source": [
        "def create_square(**kwargs) -> np.ndarray:\n",
        "  # This function creates the trace of an object moving on the unit square with\n",
        "  # unit speed.\n",
        "  # See the functions \"generate_sampling_times\" and \"create_template_trace\" for\n",
        "  # keyword-arguments.\n",
        "  return concat_traces(trace_configs = [\n",
        "      ('linear', {'duration': 1.0, 'init_pos': (1.0, 0.0), 'velocity': (0.0, 1.0)}),\n",
        "      ('linear', {'duration': 2.0, 'velocity': (-1.0, 0.0)}),\n",
        "      ('linear', {'duration': 2.0, 'velocity': (0.0, -1.0)}),\n",
        "      ('linear', {'duration': 2.0, 'velocity': (1.0, 0.0)}),\n",
        "      ('linear', {'duration': 1.0, 'velocity': (0.0, 1.0)})\n",
        "    ],\n",
        "    **kwargs\n",
        "  )\n",
        "\n",
        "ScenarioResult = Tuple[np.ndarray, np.ndarray]\n",
        "\n",
        "def compute_labels(times: np.ndarray,\n",
        "                   regular_duration: float,\n",
        "                   anomaly_duration: float) -> ScenarioResult:\n",
        "  # This function returns a boolean array indicating whether observations in\n",
        "  # the given synthetic dataset are anomalous or not.\n",
        "  # The argument \"times\" denotes the array of observation timestamps.\n",
        "  # Arguments \"regular_duration\" and \"anomaly_duration\" specify the time\n",
        "  # interval for which the object temporarily deviates from its regular course.\n",
        "  return np.logical_and(times < regular_duration + anomaly_duration,\n",
        "                        times > regular_duration)\n",
        "\n",
        "def scenario_change_speed(regular_cycles: int,\n",
        "                          new_speed: float,\n",
        "                          **kwargs) -> ScenarioResult:\n",
        "  # This function constructs a scenario where the object first makes several\n",
        "  # tours on the unit circle, then changes its speed for a while, and finally\n",
        "  # reverts back to its normal motion.\n",
        "\n",
        "  # The argument \"regular_cycles\" specifies how many cycles the object goes\n",
        "  # through before the anomalous section of the trace.\n",
        "  # The argument \"new_speed\" specifies the speed in the anomalous section of\n",
        "  # the trace.\n",
        "  # See the functions \"generate_sampling_times\" and \"create_template_trace\" for\n",
        "  # keyword-arguments.\n",
        "  anomaly_duration = np.pi\n",
        "  regular_duration = 2.0 * np.pi * regular_cycles\n",
        "  trace = concat_traces(trace_configs = [\n",
        "      ('circular', {'duration': regular_duration, 'init_pos': (1.0, 0.0)}),\n",
        "      ('circular', {'duration': anomaly_duration, 'speed': new_speed}),\n",
        "      ('circular', {'duration': 2.0 * np.pi})\n",
        "    ],\n",
        "    **kwargs\n",
        "  )\n",
        "  return trace, compute_labels(trace[:, 0], regular_duration, anomaly_duration)\n",
        "  \n",
        "def scenario_off_circle(regular_cycles: int, **kwargs) -> ScenarioResult:\n",
        "  # This function constructs a scenario where the object first makes several\n",
        "  # tours on the unit circle, then leaves the unit circle and moves on the unit\n",
        "  # square for a while, and finally reverts back to its normal motion.\n",
        "  \n",
        "  # The argument \"regular_cycles\" specifies how many cycles the object goes\n",
        "  # through before the anomalous section of the trace.\n",
        "  # See the functions \"generate_sampling_times\" and \"create_template_trace\" for\n",
        "  # keyword-arguments.\n",
        "  anomaly_duration = 8.0\n",
        "  regular_duration = 2.0 * np.pi * regular_cycles\n",
        "  trace = concat_traces(trace_configs = [\n",
        "      ('circular', {'duration': regular_duration, 'init_pos': (1.0, 0.0)}),\n",
        "      ('immediate', {'data': create_square(**{**kwargs, 'noise_magnitude': 0.0})}),\n",
        "      ('circular', {'duration': 2.0 * np.pi})\n",
        "    ],\n",
        "    **kwargs\n",
        "  )\n",
        "  return trace, compute_labels(trace[:, 0], regular_duration, anomaly_duration)\n",
        "  \n",
        "def scenario_off_square(regular_cycles: int, **kwargs) -> ScenarioResult:\n",
        "  # This function constructs a scenario where the object first makes several\n",
        "  # tours on the unit square, then leaves the unit square and moves on the unit\n",
        "  # circle for a while, and finally reverts back to its normal motion.\n",
        "\n",
        "  # The argument \"regular_cycles\" specifies how many cycles the object goes\n",
        "  # through before the anomalous section of the trace.\n",
        "  # See the functions \"generate_sampling_times\" and \"create_template_trace\" for\n",
        "  # keyword-arguments.\n",
        "  anomaly_duration = 2.0 * np.pi\n",
        "  regular_duration = 8.0 * regular_cycles  \n",
        "  make_square = lambda: create_square(**{**kwargs, 'noise_magnitude': 0.0})  \n",
        "  trace = concat_traces(trace_configs = [\n",
        "      *[('immediate', {'data': make_square()}) for _ in range(regular_cycles)],\n",
        "      ('circular', {'duration': anomaly_duration}),\n",
        "      ('immediate', {'data': make_square()})\n",
        "    ],\n",
        "    **kwargs\n",
        "  )\n",
        "  return trace, compute_labels(trace[:, 0], regular_duration, anomaly_duration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYjoRllz2XvF"
      },
      "source": [
        "# Create three example scenarios:\n",
        "trace_rev, anomalies_rev = scenario_change_speed(regular_cycles = 5,\n",
        "                                                 new_speed = -1.0,\n",
        "                                                 avg_sampling_interv = 0.1,\n",
        "                                                 noise_magnitude = 0.02)\n",
        "trace_cs, anomalies_cs = scenario_off_circle(regular_cycles = 5,\n",
        "                                             avg_sampling_interv = 0.1,\n",
        "                                             noise_magnitude = 0.02)\n",
        "trace_sc, anomalies_sc = scenario_off_square(regular_cycles = 5,\n",
        "                                             avg_sampling_interv = 0.1,\n",
        "                                             noise_magnitude = 0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxl0LsUm4yYZ"
      },
      "source": [
        "# You can use this function to visualize a scenario:\n",
        "create_animation(trace_sc,\n",
        "                 min_x = -1.1,\n",
        "                 max_x = 1.1,\n",
        "                 min_y = -1.1,\n",
        "                 max_y = 1.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voSJH86MM1fw"
      },
      "source": [
        "## Skeleton Code\n",
        "\n",
        "Below, we define the base class you will derive from when formulating your algorithms. There are also a few utility functions to help get things going.\n",
        "\n",
        "Here is some food for thought, just in case it helps:\n",
        "- Both parametric and non-parametric approaches are applicable here, each having its advantages and drawbacks. Consider experimenting with both.\n",
        "- For parametric approaches, figuring out the appropriate update rule for your parameters (in the `update` call) constitutes the core challenge.\n",
        "- For non-parametric approaches, you will probably store the observations you have seen so far in a data structure, and query this data structure to compute scores. The choice, implementation and/or the usage of this data structure constitutes the core challenge.\n",
        "- Do not forget that you need not reinvent the wheel; feel free to use any Python library of your choice to make your life easy.\n",
        "\n",
        "We are purposefully refraining from providing an entire simulation/development environment because we want to see **your genuine approach in structuring your work**. You will probably need to:\n",
        "- Create workflows to evaluate/compare candidate algorithms without choosing threshold values (for example, via a metric such as AUC).\n",
        "- Create an evaluation framework to choose the best hyper-parameters for a given algorithm. We suggest designing this framework so that you can use multiple datasets when evaluating algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkDh_kjMTVcL",
        "outputId": "13b0fbfb-38de-4fb4-f492-15ba825548c9"
      },
      "source": [
        "import abc\n",
        "\n",
        "# You will extend this class to implement algorithm(s).\n",
        "class BaseAlgorithm(abc.ABC):\n",
        "  def __init__(self, warmup_duration: float):\n",
        "    self.last_ts = -float('inf')\n",
        "    self.warmup_ts = warmup_duration\n",
        "\n",
        "  def process(self, ts: float, *values: float) -> Optional[float]:\n",
        "    # This function processes every incoming observation. The function returns\n",
        "    # NaN scores until the algorithm warms up.\n",
        "\n",
        "    # Verify arguments:\n",
        "    if ts < self.last_ts:\n",
        "      raise ValueError('Non-monotonic timestamp, check the data!')\n",
        "    # Save the timestamp when warm-up will complete:\n",
        "    if self.last_ts == -float('inf'):\n",
        "      self.warmup_ts += ts\n",
        "    score = self.update(ts, *values)\n",
        "    self.last_ts = ts    \n",
        "    if ts >= self.warmup_ts:\n",
        "      return score\n",
        "    return float('nan')\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def update(self, ts: float, *values: float) -> float:\n",
        "    # Override this function to process every incoming observation.\n",
        "    raise NotImplementedError()\n",
        "\n",
        "# Below, we give a memoryless algorithm that overfits to the unit circle.\n",
        "# NOTE: This is an example of what NOT to do! We are just providing this to\n",
        "#       demonstrate things.\n",
        "class StupidAlgorithm(BaseAlgorithm):\n",
        "  def update(self, ts: float, *values: float) -> float:\n",
        "    radius = np.sqrt(np.dot(values, values))\n",
        "    return abs(radius - 1.0)\n",
        "\n",
        "def get_scores(trace: np.ndarray, algo: BaseAlgorithm) -> np.ndarray:\n",
        "  # This function runs the given trace (the argument \"trace\") through the given\n",
        "  # algorithm (the argument \"algo\") and outputs an array of scores.\n",
        "  return np.asarray([algo.process(*observation) for observation in trace])\n",
        "\n",
        "def get_alerts(scores: np.ndarray, threshold: float) -> np.ndarray:\n",
        "  # This function compares the given score array (the argument \"scores\") with\n",
        "  # the given threshold value (the argument \"threshold\") to decide alerts.\n",
        "  return scores >= threshold\n",
        "\n",
        "def compute_fpnr(trace: np.ndarray,\n",
        "                 anomalies: np.ndarray,\n",
        "                 algo: BaseAlgorithm,\n",
        "                 threshold: float) -> Tuple[float, float]:\n",
        "  # This function computes the false positive rate and the false negative rate\n",
        "  # of the given algorithm (the argument \"algo\") for the given dataset (the\n",
        "  # argument \"trace\") under a threshold value (the argument \"threshold\").\n",
        "  # The argument \"anomalies\" give the ground truth we use in our calculations.\n",
        "  scores = get_scores(trace, algo)\n",
        "  alerts = get_alerts(scores, threshold)\n",
        "  regulars = np.logical_not(anomalies)\n",
        "  false_positives = np.logical_and(alerts, regulars)\n",
        "  false_negatives = np.logical_and(np.logical_not(alerts), anomalies)\n",
        "  fpr = np.sum(false_positives) / np.sum(regulars)\n",
        "  fnr = np.sum(false_negatives) / np.sum(anomalies)\n",
        "  return fpr, fnr\n",
        "\n",
        "# See that the example algorithm works OK for the scenario we overfitted it to:\n",
        "print(compute_fpnr(trace_cs, anomalies_cs, StupidAlgorithm(warmup_duration = 1.0), 0.025))\n",
        "# But it fails miserably in one of the other scenarios:\n",
        "print(compute_fpnr(trace_sc, anomalies_sc, StupidAlgorithm(warmup_duration = 1.0), 0.025))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.16414141414141414, 0.2127659574468085)\n",
            "(0.7728155339805826, 0.6964285714285714)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pVH9lsI_Rdt"
      },
      "source": [
        "## Bonus\n",
        "\n",
        "We did not consider expiring observations (i.e. aging observations we want to neglect or de-emphasize) so far. To account for this, we would need to add another hyper-parameter (e.g. `memory_duration` or `half_life`) to our algorithm(s).\n",
        "\n",
        "A simple yet typical approach is to maintain a queue of active (i.e. recent enough with respect to this new hyper-parameter) observations and update our algorithms' state whenever an observation expires. One can do this in the `update` function by consulting this queue and doing the appropriate calculations to \"unlearn\" observations we remove from the queue.\n",
        "\n",
        "IRL, expiring aging observations is important for two reasons:\n",
        "1. You can not implement non-parametric techniques in practice unless you limit the memory.\n",
        "2. Drifts and/or slow structural changes in systems/datasets require us to expire aging observations to make certain techniques adaptable.\n",
        "\n",
        "For example, imagine a scenario where:\n",
        "- The object slowly drifts towards some direction during its regular motion,\n",
        "- After some time, it suddenly reverts to its original course.\n",
        "\n",
        "In order to recognize such behaviors as anomalous with elementary techniques, expiring aging observations using the above approach can come in handy.\n",
        "\n",
        "If you had fun working on the above challenge, feel free to construct such scenarios and explore how you can account for expiring observations."
      ]
    }
  ]
}